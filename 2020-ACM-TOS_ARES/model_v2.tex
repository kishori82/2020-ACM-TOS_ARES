A shared atomic storage, consisting of any number of individual objects,  can be emulated
by composing individual atomic memory objects. Therefore, herein we aim
in implementing a single atomic \textit{read/write} memory object. %on a set of servers.
{A read/write} object takes a value from a set $\valSet$. 
We assume a system consisting of four distinct sets of processes: 
a set $\wSet$ of writers, a set $\rdSet$ of readers, a set $\recSet$ of 
reconfiguration clients, and a set $\srvSet$ of servers. Let $\cSet = \wSet \cup\rdSet\cup\recSet$ 
be the set of clients. Servers host data elements (replicas or encoded data fragments).
Each writer is allowed to modify the value of a shared object, and each reader is allowed to obtain 
the value of that object. Reconfiguration clients attempt  to introduce new 
 configuration of servers to the 
system in order to mask transient errors and to ensure the longevity of the service. 
Processes communicate via \myemph{messages} through 
\myemph{asynchronous}, and \myemph{reliable} channels. 
%%Let $\idSet = \cSet\cup\reconSet\cup\srvSet$. 
%In a read/write object implementation, we assume that the object may take a value from a set $\valSet$. 
%Each writer is allowed to modify the value of the object, and each reader is allowed to obtain 
%the value of the object. Servers host data elements (replicas or encoded data fragments).
%%maintain encoded elements of the redundant object.
%
%We assume an \myemph{asynchronous} environment, where processes communicate
%by exchanging messages. The writer, any subset of readers, and up to 
%$f$ servers may \myemph{crash} without any notice.
%\ares{} allows the set of server host to be modified during the course of an execution for 
%masking transient or permanent failures of servers and preserve the longevity of the service.  
%\kmk{In the paper, we are interested only  in the fair executions of any algorithm.}

\myparagraph{Configurations.} 
\kmk{A \textit{configuration},  with a unique identifier from a set $\confSet$, is a data type that 
	describes the finite set of servers that are used to implement the atomic storage service. In our setting, 
	each configuration is also used to describe the way the servers are grouped into intersecting sets, called 
	\textit{quorums}, the consensus instance that is used as an external service to determine the next configuration, 
	and a set of data access primitives that specify 
	the interaction of the clients and servers in the configuration %for implementing the read/write operations 
	(see Section \ref{ssec:dap}). 
%	A configuration refers In our setting, a \textit{configuration} consists of a finite set of servers, describes 
%	the organization of the servers in intersecting sets, and specifies a consensus algorithm
%	implemented by the servers of the configuration.
}
	%Informally, in our setting, a configuration consists of a finite set of servers, each with an unique identifier, that
%$(i)$ collectively implements an atomic object; $(ii)$ implements an underlying quorum-based algorithm for implmenting the
%atomic object; and $(iii)$ implements some distributed consensus algorithm}
More formally, a configuration,  
%identified by a unique identifier 
 $c\in\confSet$, consists of: 
 %is a data type that describes explicitly: 
$(i)$ $\servers{c}\subseteq\srvSet$: a set of server identifiers; %that {belong} in $c$; 
$(ii)$ $\quorums{c}$: the set of quorums on $\servers{c}$, s.t. $\forall Q_1,Q_2\in\quorums{c}, Q_1,Q_2\subseteq\servers{c}$ and $Q_1\cap Q_2\neq \emptyset$; 
%$(iii)$ \nn{an underlying algorithm, $\algo{c}$, that implements an atomic memory (including related parameters);}
$(iii)$ $\dap{c}$: the set of primitives (operations at level lower than reads or writes) that clients in $\idSet$ may invoke on $\servers{c}$; 
% an underlying algorithm that implements atomic memory in $\servers{c}$, including related parameters; 
and $(iv)$ $\consensus{c}$: a consensus instance with the values from $\confSet$, %the set of all configuration identifiers, 
implemented and running on top of the servers in $\servers{c}$.
%, the set of servers in 
%some $c \in \confSet$ is denoted by $\servers{c}$.
We refer to a server $s \in \servers{c}$ as a \myemph{member} of  configuration $c$.
% and (iii) the data access primitives that are used to access the data in those servers.  
%we can formally define a quorum system $\quorums{c}$, with $c\in\confSet$, as follows:
%Consider a configuration identifier $c$.
%We define as $\servers{c}=\bigcup_{Q\in\quorums{c}} Q$ the set of servers that belong in the quorums
%f a quorum system $\quorums{c}$. 
%We refer to a server $s$ as a \myemph{member} of a configuration $c$ if $s\in \servers{c}$.
%
%\paragraph{Notations and Definitions}  We denote by $\mathcal{W}$, $\mathcal{R}$ and $\mathcal{S}$,  the set of writers, readers and servers, respectively.
%We denote by $\confSet$ the set of possible configurations of the systems. A configuration $c \in \confSet$ has a unique the configuration identifier   $c.conf$; a set of servers $c.servers$, such that, $c.Servers \subseteq \mathcal{S}$,
%$cseq[i].status\in\{F,P\}$, the pending or finalized status 
%
%The writer process $w$ may invoke write operations on the atomic object 
%by calling the $\act{write}(v)$ function whereas each reader may invoke 
%a read operation by calling the $\act{read}$ function. Each write event 
%returns an acknowledgement when successfully carried-out, whereas each read 
%operation returns the value of the atomic object. We assume that an 
%
%\nn{
%\blue{We assume that there is an instance of consensus protocol running on servers in $\srvSet{s}$.}
\nn{ The consensus instance $\consensus{c}$ in each configuration $c$ is used as a service that 
	returns the identifier of the configuration that follows $c$. }   
%	
%	
%	Note that due to the asynchrony of the channels the consensus instance $\consensus{c}$ may not terminate~\cite{FLP85}.
%	As we will see below this may affect the liveness of an ongoing reconfiguration and the liveness of read/write operations when 
%	not enough servers remain alive in the current configuration. The safety of read/write operations, however, is not affected.} 

\myparagraph{Executions.} An algorithm $A$ is a collection of processes, where process $A_p$
is assigned to \nnfix{process} $p\in\idSet\cup\srvSet$. The \textit{state}, of a process $A_\pr$ is determined over a
set of state variables, and the state $\state$ of $A$ is a vector that contains the state of
each process. Each process $A_\pr$ implements a set of actions. When an action $\acts{}$ occurs 
it causes the state of $A_\pr$ to change, say from 
some state $\state_p$ to some different state $\state_p'$. We call the triple $\tup{\state_p, \acts{}, \state_p'}$
a \textit{step} of $A_\pr$. Algorithm $A$ performs a step, when some process $A_\pr$ performs a step.
%: (i) receives a
%message, (ii) performs local computation, (iii) sends a message. 
%Each such action
%causes the state at $p$ to change. 
An action $\acts{}$ is \textit{enabled} in a state $\state$ if $\exists$ a step $\tup{\state, \acts{}, \state'}$
	to some state $\state'$.
An \textit{execution} is an alternating sequence of states
and actions of $A$ starting with the initial state and ending in a state. 
An execution $\EX$ 
%is \textit{well-formed} if any process invokes one operation at a time and it is 
\textit{fair} if enabled actions perform a step infinitely often. In the rest of the paper 
we consider executions that are fair and well-formed. A process
$\pr$ \textit{crashes} in an execution if it stops taking steps; otherwise $p$ is \textit{correct} or \textit{non-faulty}.
We assume a function $c.\mathcal{F}$ to describe the failure model of a configuration $c$.


\myparagraph{\kmk{Reconfigurable Atomic Read/Write Objects.}} \kmk{
	A reconfigurable atomic object supports three operations:  $\act{read}()$, $\act{write}(v)$ and $\act{reconfig}(c)$.
	A \act{read}() operation returns the value of the atomic object, $\act{write}(v)$ attempts to modify the value of 
	the object to $v\in\valSet$, and the $\act{reconfig}(c)$ that attempts to install a new configuration $c\in\confSet$.
 We assume \textit{well-formed} executions where each client may invoke \nnfix{one} operation ($\act{read}()$, $\act{write}(v)$ or $\act{reconfig}(c)$) 
at a time. 
%To allow reconfiguration of our atomic memory service, in addition to the usual reads or writes,  we assume an additional operation $reconfig(c)$, where $c$ is a new configuration,  that reconfig clients can invoke to install new configuration of servers or storage nodes.
%
% Reconfig operation invoked at a client,  upon completion,  returns  an {\it ok} response. 
% Note that a reconfig client does not invoke
% a new reconfig operation unless the previously invoked reconfig operations at it is complete.
% Note that any time serveral reconfig operations can be invoked at various clients, or while other read/write operations are underway.
%We need these properties in order to cope with the asynchronous nature of the network.
}


An implementation of a \act{read}/\act{write} or a \act{reconfig} operation contains an \textit{invocation} action 
	(such as a call to a procedure) and a \textit{response} action (such as a
	return from the procedure). An operation $\op$ is \textit{complete} in an execution, if it
	contains both the invocation and the \textit{matching} response actions for $\op$; otherwise $\op$
	is \textit{incomplete}. 
	We say that an operation $\op$ \textit{precedes} an operation $\op'$ in an execution $\EX$,
	denoted by $\op\bef\op'$, if the response step of $\op$ appears before the invocation
	step of $\op'$ in $\EX$. Two operations are \textit{concurrent} if neither precedes the other.
	An implementation $A$ of a read/write object satisfies the atomicity \nnfix{(linearizability \cite{HW90})} property
	if the following holds \cite{Lynch1996}. Let the set $\Pi$ contain all complete \nnfix{read/write} operations in
	any well-formed execution of $A$. 
	Then %for operations in $\Pi$ 
	there exists an irreflexive partial ordering $\prec$ satisfying the
	following:	
	\begin{itemize}
		%\item [\em P1.] No operation has infinitely many other 
		%			operations ordered before it.
		\item [\bf A1.] 
		%					The partial order is consistent with the 
		%					external order of invocation and responses, that is, there do 
		%					not exist operations $\op_1$ and $\op_2$, 
		%					such that $\op_1$ completes before $\op_2$ starts, 
		%					yet $\op_2 \prec \op_1$.
		For any operations $\pi_1$ and $\pi_2$ in $\Pi$,  if $\pi_1\bef\pi_2$, then it
		cannot be the case that $\pi_2\prec \pi_1$.
		\item[\bf A2.] 
		If $\pi\in\Pi$ is a write operation and $\pi'\in\Pi$ is any \nnfix{read/write} operation,  
		then either $\pi\prec \pi'$ or $\pi'\prec \pi$.
		%					All write operations are totally 
		%					ordered and every read operation is ordered with respect 
		%					to all the writes.
		\item[\bf A3.] 
		The value returned by a read operation is the value 
		written by the last preceding write operation according to
		$\prec$ (or the initial value if there is no such write).
		%		Every read operation ordered after any writes returns
		%the value of the last write preceding it in the partial order, and any
		%read operation ordered before all writes returns the initial value
		%of the object.
\end{itemize}


\myparagraph{Storage and Communication Costs.} We are interested in the \myemph{complexity} of each
read and write operation. The complexity of each operation $\op$ invoked by a process 
$\pr$, is measured with respect to three metrics, during the interval between the invocation 
and the response of $\op$: $(i)$ \myemph{number of communication round}, accounting the number of messages 
exchanged during $\op$, $(ii)$ \myemph{storage efficiency} (storage cost), accounting the maximum storage requirements for 
any single object at the servers during $\op$, and  $(iii)$ \myemph{message bit complexity} (communication cost)
which measures the size of the messages used during $\op$. 

We define the total storage cost as the size of the
data stored across all servers, at any point during the execution of the algorithm. The
communication cost associated with a read or write operation is the size of the total data that
gets transmitted in the messages sent as part of the operation. We assume that metadata,
such as version number, process ID, etc. used by various operations is of negligible size, and
is hence ignored in the calculation of storage and communication cost. Further, we normalize
both costs with respect to the size of the value $v$; in other words, we compute the costs
under the assumption that $v$ has size $1$ unit.

 \myparagraph{{\bf Erasure Codes}.} %In algorithm \treas{}, 
 We use an $[n, k]$  linear MDS code ~\cite{verapless_book} over a finite field $\mathbb{F}_q$ to encode and store the value $v$ among the $n$ servers. An $[n, k]$ MDS code has the property that any $k$ out of the $n$ coded elements can be used to recover (decode) the value $v$. For encoding, $v$ is divided
 %\footnote{In practice $v$ is a file, which is divided into many stripes based on the choice of the code, various stripes are individually encoded and stacked against each other. We omit details of represent-ability of $v$ by a sequence of symbols of $\mathbb{F}_q$, and the mechanism of data striping, since these are fairly standard in the coding theory literature.} 
 into $k$ elements $v_1, v_2, \ldots v_k$ with each element having  size $\frac{1}{k}$ (assuming size of $v$ is $1$). The encoder takes the $k$ elements as input and produces $n$ coded elements $e_1, e_2, \ldots, e_n$ as output, i.e., $[e_1, \ldots, e_n] = \Phi([v_1, \ldots, v_k])$, where $\Phi$ denotes the encoder. For ease of notation, we simply write $\Phi(v)$ to mean  $[e_1, \ldots, e_n]$. The vector $[e_1, \ldots, e_n]$ is  referred to as the codeword corresponding to the value $v$. Each coded element $c_i$ also has  size $\frac{1}{k}$. In our scheme we store one coded element per server. We use $\Phi_i$ to denote the projection of $\Phi$ on to the $i^{\text{th}}$ output component, i.e., $e_i = \Phi_i(v)$. Without loss of generality, we associate the coded element $e_i$ with server $i$, $1 \leq i \leq n$.

%\myparagraph{Storage and Communication Cost.} We define the total storage cost as the size of the
%data stored across all servers, at any point during the execution of the algorithm. The
%communication cost associated with a read or write operation is the size of the total data that
%gets transmitted in the messages sent as part of the operation. We assume that metadata,
%such as version number, process ID, etc. used by various operations is of negligible size, and
%is hence ignored in the calculation of storage and communication cost. Further, we normalize
%both the costs with respect to the size of the value v; in other words, we compute the costs
%under the assumption that v has size 1 unit.

%A communication round-trip
%(or simply round) is more formally defined in the following definition that appeared in \cite{CDGL04,GNS09,GNS08}:

%\begin{definition}\label{def:com}
%Process $p$ performs a communication round during operation $\op$ 
%if all of the following hold:
%\begin{enumerate}
%   \item $p$ sends request messages that are a part of $\op$ 
%         to a set of processes,
%	\item any process $q$ that receives a request message from $p$ for 
%         operation $\op$, replies
%% to it with an acknowledgment 
%%         message, at the first possible convenience and 
%without delay, i.e. without waiting for any other messages before 
%replying to $\op$.
%   \item when process $p$ receives ``enough'' replies it 
%terminates the round
%%, orreturns or repeats a communication round
%         %then makes a local decision about termination of the phase.
%\end{enumerate}
%\end{definition}
%
%At the end of a communication round process $\pr$ may complete $\op$
%or start a new round. Operation $\op$ is \myemph{fast} \cite{CDGL04} if it completes after 
%its first communication round; an implementation is fast if in each execution
%all operations are fast.
\myparagraph{{\bf Tags.}}
We use logical tags to order operations. A tag $\tg{}$ is defined as a pair $(z, w)$, where $z \in \mathbb{N}$ and $w \in \mathcal{W}$, an ID of a writer.
Let $\mathcal{T}$ be the set of all tags.
Notice that tags could be defined in any totally ordered domain and given that this domain is countably infinite, then 
there can be a direct mapping to the domain we assume. 
% and we denote by  $\mathcal{T}$  the set of all possible tags. 
For any  $\tg{1}, \tg{2} \in \mathcal{T}$ we define  $\tg{2} > \tg{1}$ if $(i)$ $\tg{2}.z > \tg{1}.z$ or $(ii)$ $\tg{2}.z = \tg{1}.z$ and $\tg{2}.w > \tg{1}.w$.
